{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spell Checker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0ULIqCeiXR1Qfyc+ksqdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyaabdul/DataCamp/blob/master/Spell_Checker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWFpEbGbMUTf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open('/content/text_tere_liye_fb08042020.txt').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6vX3P52ghN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad2462e0-d674-4fc2-9249-c0423dcc35b7"
      },
      "source": [
        "# !pip install sastrawi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.6/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjtdsIx3MYqk",
        "colab_type": "code",
        "outputId": "681cb389-54ec-45f5-decd-fab1f91d275a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import re\n",
        "from nltk.util import ngrams\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "\n",
        "#ubah menjadi lower text\n",
        "text = text.lower()\n",
        "#hilangkan karakter selain huruf dan angka\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "#hilagkan \\n pada kalimat\n",
        "text = re.sub(r'\\n','',text)\n",
        "\n",
        "#buat stopword\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword = factory.create_stop_word_remover()\n",
        "\n",
        "#hilangkan stopword pada kalimat\n",
        "stop = stopword.remove(text)\n",
        "\n",
        "#buat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "#stemming kata\n",
        "stem = stemmer.stem(stop)\n",
        "\n",
        "#split kalimat menjadi token per kata\n",
        "tokens = [token for token in stem.split(\" \") if token != \"\"]\n",
        "\n",
        "#n-gram 1 token/kata\n",
        "token1 = list(ngrams(tokens,1))\n",
        "#n-gram 2 token/kata\n",
        "token2 = list(ngrams(tokens,2))\n",
        "#n-gram 3 token/kata\n",
        "token3 = list(ngrams(tokens,3))\n",
        "\n",
        "\n",
        "print(token1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('buat',), ('kalian',), ('tiap',), ('hari',), ('kritik',), ('perintah',), ('sudah',), ('kalian',), ('isi',), ('setor',), ('spt',), ('2019',), ('buat',), ('kalian',), ('tiap',), ('hari',), ('puja',), ('muji',), ('perintah',), ('lantas',), ('ngamuk',), ('baca',), ('tulis',), ('kritik',), ('perintah',), ('sudah',), ('kalian',), ('isi',), ('setor',), ('spt',), ('2019',), ('salah',), ('satu',), ('wajib',), ('warga',), ('negara',), ('bayar',), ('pajak',), ('benci',), ('minta',), ('ampun',), ('lihat',), ('fakta',), ('juta',), ('buku2',), ('baja',), ('jual',), ('shopee',), ('tokopedia',), ('bukalapak',), ('lazada',), ('mau',), ('marah',), ('ubun2',), ('lihat',), ('juta',), ('orang',), ('tonton',), ('film2',), ('buku',), ('sy',), ('ilegal',), ('lagi',), ('juta',), ('ebook',), ('ilegal',), ('bagi',), ('whatsapp',), ('sy',), ('bahkan',), ('lapor',), ('menteri',), ('periode',), ('lalu',), ('soal',), ('minta',), ('karya',), ('sy',), ('lindung',), ('tetap',), ('hak',), ('sy',), ('wni',), ('tdk',), ('lindung',), ('marah',), ('sy',), ('tetap',), ('bayar',), ('pajak',), ('tetap',), ('lapor',), ('setor',), ('spt',), ('2019',), ('sakit',), ('hati',), ('sy',), ('tetap',), ('bayar',), ('pajak',), ('paham',), ('cinta',), ('negara',), ('itu',), ('tidak',), ('lihat',), ('berapa',), ('banyak',), ('postingan',), ('kalian',), ('media',), ('sosial',), ('lihat',), ('berapa',), ('banyak',), ('kontribusi',), ('kalian',), ('negara',), ('itu',), ('salah',), ('satu',), ('adalah',), ('pajak',), ('jadi',), ('ayo',), ('deadline',), ('tambah',), ('nya',), ('habis',), ('lapor',), ('spt',), ('2019',), ('kalian',), ('henti',), ('dulu',), ('berisik',), ('jadi',), ('buzzer',), ('medsos',), ('puja',), ('puji',), ('hampar',), ('juga',), ('balik',), ('henti',), ('dulu',), ('nulis',), ('kritik',), ('kalau',), ('lapor',), ('spt',), ('2019',), ('monggo',), ('lanjut',), ('jadi',), ('buzzer',), ('kritikus',)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iLbg3vh0PKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1d647da1-4b54-4f1f-a19e-e54f861d6c69"
      },
      "source": [
        "cekkata=[]\n",
        "x=0\n",
        "\n",
        "for cek in token1:\n",
        "  # cek = re.sub(r'\\W','',cek)\n",
        "  cekkata.append(cek)\n",
        "\n",
        "# x=0\n",
        "# for c in cekkata:\n",
        "#   cekkata[x] = re.sub(r'\\W','',cekkata[x])\n",
        "#   print(cekkata[x])\n",
        "  x=x+1\n",
        "  \n",
        "\n",
        "\n",
        "print(cekkata)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('buat',), ('kalian',), ('tiap',), ('hari',), ('kritik',), ('perintah',), ('sudah',), ('kalian',), ('isi',), ('setor',), ('spt',), ('2019',), ('buat',), ('kalian',), ('tiap',), ('hari',), ('puja',), ('muji',), ('perintah',), ('lantas',), ('ngamuk',), ('baca',), ('tulis',), ('kritik',), ('perintah',), ('sudah',), ('kalian',), ('isi',), ('setor',), ('spt',), ('2019',), ('salah',), ('satu',), ('wajib',), ('warga',), ('negara',), ('bayar',), ('pajak',), ('benci',), ('minta',), ('ampun',), ('lihat',), ('fakta',), ('juta',), ('buku2',), ('baja',), ('jual',), ('shopee',), ('tokopedia',), ('bukalapak',), ('lazada',), ('mau',), ('marah',), ('ubun2',), ('lihat',), ('juta',), ('orang',), ('tonton',), ('film2',), ('buku',), ('sy',), ('ilegal',), ('lagi',), ('juta',), ('ebook',), ('ilegal',), ('bagi',), ('whatsapp',), ('sy',), ('bahkan',), ('lapor',), ('menteri',), ('periode',), ('lalu',), ('soal',), ('minta',), ('karya',), ('sy',), ('lindung',), ('tetap',), ('hak',), ('sy',), ('wni',), ('tdk',), ('lindung',), ('marah',), ('sy',), ('tetap',), ('bayar',), ('pajak',), ('tetap',), ('lapor',), ('setor',), ('spt',), ('2019',), ('sakit',), ('hati',), ('sy',), ('tetap',), ('bayar',), ('pajak',), ('paham',), ('cinta',), ('negara',), ('itu',), ('tidak',), ('lihat',), ('berapa',), ('banyak',), ('postingan',), ('kalian',), ('media',), ('sosial',), ('lihat',), ('berapa',), ('banyak',), ('kontribusi',), ('kalian',), ('negara',), ('itu',), ('salah',), ('satu',), ('adalah',), ('pajak',), ('jadi',), ('ayo',), ('deadline',), ('tambah',), ('nya',), ('habis',), ('lapor',), ('spt',), ('2019',), ('kalian',), ('henti',), ('dulu',), ('berisik',), ('jadi',), ('buzzer',), ('medsos',), ('puja',), ('puji',), ('hampar',), ('juga',), ('balik',), ('henti',), ('dulu',), ('nulis',), ('kritik',), ('kalau',), ('lapor',), ('spt',), ('2019',), ('monggo',), ('lanjut',), ('jadi',), ('buzzer',), ('kritikus',)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf0oo4yeCDb5",
        "colab_type": "code",
        "outputId": "8819d6ac-98fb-4668-ecd4-32916b9f3868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('/content/kata_dasar.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    # \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    # \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    # \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    # \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    # \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)] # [('', 'kemarin'), ('k', 'emarin'), ('ke', 'marin'), dst]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R] # ['emarin', 'kmarin', 'kearin', dst]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] # ['ekmarin', 'kmearin', 'keamrin', dst]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # ['aemarin', 'bemarin', 'cemarin', dst]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters] # ['akemarin', 'bkemarin', 'ckemarin', dst]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    # \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "\n",
        "cekkata=[]\n",
        "x=0\n",
        "\n",
        "for cek in token1:\n",
        "  cekkata.append(cek)\n",
        "  x=x+1\n",
        "\n",
        "x=0\n",
        "for cek in cekkata:\n",
        "  kata = str(cekkata[x])\n",
        "  kata = re.sub(r'\\W','',kata) \n",
        "  \n",
        "  if correction(kata) != kata:\n",
        "    print('kata typo : ', kata)\n",
        "    print('koreksi : ', correction(kata))\n",
        "  x=x+1\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kata typo :  spt\n",
            "koreksi :  sut\n",
            "kata typo :  muji\n",
            "koreksi :  tuji\n",
            "kata typo :  ngamuk\n",
            "koreksi :  nyamuk\n",
            "kata typo :  spt\n",
            "koreksi :  sut\n",
            "kata typo :  buku2\n",
            "koreksi :  bukut\n",
            "kata typo :  shopee\n",
            "koreksi :  stoper\n",
            "kata typo :  tokopedia\n",
            "koreksi :  logopedia\n",
            "kata typo :  lazada\n",
            "koreksi :  lahad\n",
            "kata typo :  ubun2\n",
            "koreksi :  ubung\n",
            "kata typo :  film2\n",
            "koreksi :  film\n",
            "kata typo :  sy\n",
            "koreksi :  y\n",
            "kata typo :  ebook\n",
            "koreksi :  encok\n",
            "kata typo :  sy\n",
            "koreksi :  y\n",
            "kata typo :  sy\n",
            "koreksi :  y\n",
            "kata typo :  sy\n",
            "koreksi :  y\n",
            "kata typo :  wni\n",
            "koreksi :  ani\n",
            "kata typo :  tdk\n",
            "koreksi :  tak\n",
            "kata typo :  sy\n",
            "koreksi :  y\n",
            "kata typo :  spt\n",
            "koreksi :  sut\n",
            "kata typo :  sy\n",
            "koreksi :  y\n",
            "kata typo :  nya\n",
            "koreksi :  nia\n",
            "kata typo :  spt\n",
            "koreksi :  sut\n",
            "kata typo :  dulu\n",
            "koreksi :  bulu\n",
            "kata typo :  buzzer\n",
            "koreksi :  buster\n",
            "kata typo :  medsos\n",
            "koreksi :  meson\n",
            "kata typo :  dulu\n",
            "koreksi :  bulu\n",
            "kata typo :  nulis\n",
            "koreksi :  nudis\n",
            "kata typo :  spt\n",
            "koreksi :  sut\n",
            "kata typo :  monggo\n",
            "koreksi :  gonggo\n",
            "kata typo :  buzzer\n",
            "koreksi :  buster\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}